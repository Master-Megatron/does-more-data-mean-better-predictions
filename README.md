#  Does More Data = Better Predictions?

<div align="center">

![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)
![scikit-learn](https://img.shields.io/badge/scikit--learn-latest-orange.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-complete-success.svg)

**Debunking the myth: "More data always means better predictions"**

*A practical experiment exploring the surprising relationship between sample size and prediction accuracy*

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“Š Key Findings](#-key-findings) â€¢ [ğŸ’¡ The Surprise](#-the-surprise)

</div>

---

## ğŸ¤” The Question

> **"What happens to prediction errors when we increase our dataset from 100 to 10,000 samples?"**

Most people think: *"Obviously, more data = lower errors!"*  
**Reality:** ğŸ¤¯ **It's more nuanced than that...**

---

## ğŸ”¬ The Experiment

This repository contains a **single, focused experiment** that challenges common assumptions about data and machine learning:

- ğŸ“ˆ Test **5 different dataset sizes**: 100 â†’ 500 â†’ 1,000 â†’ 5,000 â†’ 10,000
- ğŸ”„ Run **100 repetitions** for statistical reliability
- ğŸ“‰ Track **two key metrics**: Average error & Variability
- ğŸ¨ Visualize the **surprising results**

**Total computational experiments: 500 models trained!**

---

## ğŸ’¡ The Surprise

### What Most People Expect:
```
More Data â†’ Lower Prediction Error âœ“
```

### What Actually Happens:
```
More Data â†’ Same Average Error ğŸ˜®
More Data â†’ Much Lower Variability âœ“âœ“âœ“
```

**The Insight:**  
Sample size affects **PRECISION** (consistency), not **ACCURACY** (correctness)!

---

## ğŸ“Š Key Findings

| Sample Size | Avg RMSE | Std Dev | What This Means |
|-------------|----------|---------|-----------------|
| 100 | 2.66 | 0.27 | ğŸ² High variability |
| 10,000 | 2.60 | 0.02 | ğŸ¯ Very consistent |
| **Change** | **-2%** | **-91%** | **Precision â†‘, Accuracy â‰ˆ** |

### ğŸ“‰ Visual Proof

The code generates two compelling visualizations:

1. **Average RMSE vs Sample Size** - Flat line (accuracy unchanged)
2. **Std Dev vs Sample Size** - Dramatic drop (precision improved)

---

## ğŸ¯ Why This Matters

### For Data Scientists:
- âŒ Don't expect miracles from just collecting more data
- âœ… Focus on **feature quality** and **model choice** for better predictions
- âœ… Use larger samples for **reliable estimates**, not lower errors

### For Business Leaders:
- ğŸ’° **More data â‰  better predictions** (saves $$ on unnecessary data collection)
- ğŸ¯ **Better features > more rows** (invest wisely)
- ğŸ“Š Larger samples = more **confidence**, not better **accuracy**

### For Students:
- ğŸ§  Practical demonstration of **bias-variance tradeoff**
- ğŸ“š Real-world application of statistical theory
- ğŸ’» Clean, reproducible code for learning

---

## ğŸš€ Quick Start

### Prerequisites
```bash
pip install numpy pandas scikit-learn matplotlib
```

### Run the Experiment
```bash
# Clone this repo
git clone https://github.com/YOUR_USERNAME/does-more-data-mean-better-predictions.git
cd does-more-data-mean-better-predictions

# Run the experiment (takes ~2-3 minutes)
python experiment.py
```

### Expected Output
```
Processing n = 100...     âœ“ Done (RMSE=2.66)
Processing n = 500...     âœ“ Done (RMSE=2.61)
Processing n = 1000...    âœ“ Done (RMSE=2.60)
Processing n = 5000...    âœ“ Done (RMSE=2.60)
Processing n = 10000...   âœ“ Done (RMSE=2.60)

âœ“ Plot saved: analysis.png

ANSWER: Option 1 âœ“
"On average, the RMSE does not change much as n gets larger,
while the variability of RMSE does decrease."
```

---

## ğŸ“– The Story Behind This

This experiment is from **Exercise 31.2** of Rafael Irizarry's excellent book:  
[*Introduction to Data Science*](https://rafalab.dfci.harvard.edu/dsbook/)

**The specific question (Multiple choice):**

> *"Describe what you observe with the RMSE as the size of the dataset becomes larger."*

**Options:**
1. âœ… Average RMSE stays constant, variability decreases
2. âŒ RMSE decreases (law of large numbers)
3. âŒ Need even larger n to see effects
4. âŒ RMSE is not random

**This code proves Option 1 is correct through computational experiment!**

---

## ğŸ§ª The Science

### What We're Testing
- **Bivariate Normal Data**: x and y with correlation Ï = 0.5
- **Linear Regression**: Simple y ~ x model
- **50/50 Split**: Train on half, test on half
- **RMSE Metric**: Root Mean Squared Error

### Why This Design?
- âœ… **Simple enough** to understand
- âœ… **Complex enough** to reveal insights
- âœ… **Reproducible** with fixed random seed
- âœ… **Statistically rigorous** (100 reps)

---



---

## ğŸ“ Learning Outcomes

After running this experiment, you'll understand:

1. **Bias-Variance Tradeoff** (practical demonstration)
2. **Precision vs Accuracy** (not the same thing!)
3. **Sample Size Effects** (when it helps, when it doesn't)
4. **Reproducible Research** (clean code, clear results)

---

## ğŸ’¬ Discussion Questions

Thinking about running this? Consider:

1. What would happen if we changed Ï from 0.5 to 0.95?
2. Why does variability decrease but average stay constant?
3. When SHOULD we collect more data?
4. What's more valuable: more rows or better features?

**Hint:** The answers are all in the code output! ğŸ¤“



## ğŸ“š Further Reading

Want to dive deeper? Check out:

- ğŸ“– [Introduction to Data Science](https://rafalab.dfci.harvard.edu/dsbook/) - The source material
- ğŸ“Š [Bias-Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) - The theory
- ğŸ”¬ [Statistical Power](https://en.wikipedia.org/wiki/Power_of_a_test) - Why n matters for inference

---

## ğŸ“œ License

MIT License - Feel free to use this for education, teaching, or learning!

---

## ğŸ™ Acknowledgments

- **Rafael Irizarry** for the excellent textbook and exercises
- **The Data Science Community** for making education accessible
- **You** for being curious about data! ğŸŒŸ

---

<div align="center">

### â­ Star this repo if it changed your perspective on data!

**Remember:** More data makes you more *confident*, not necessarily more *correct*.

[ğŸ” Back to Top](#-does-more-data--better-predictions)

</div>

---

## ğŸ“Š Preview

### What You'll See:

**Console Output:**
```
======================================================================
ANALISIS: Apa yang terjadi dengan RMSE?
======================================================================

1. AVERAGE RMSE:
   n=100:    2.6582
   n=10000:  2.5991
   Change:   -2.22% â† HAMPIR TIDAK BERUBAH

2. STD DEV RMSE:
   n=100:    0.2669
   n=10000:  0.0247
   Change:   -90.7% â† TURUN DRASTIS
```

**Visual Output:**  
Two side-by-side plots showing:
1. Flat average RMSE (doesn't change with n)
2. Decreasing std dev (improves dramatically with n)

---

<div align="center">

**Built with â¤ï¸ for data science education**

*Because understanding > memorizing*

</div>
